# skeleton

The notebook should include runnable cells for:

Load CSV, parse dates

Implement persistence and seasonal-naive forecasts with vectorized code

Fit SARIMA using statsmodels (and pmdarima auto_arima if available)

Fit Prophet and produce forecast and diagnostics

Log baseline RMSE/MAE/MAPE to results/metrics/baseline_vs_models.csv

(Place code cells that mirror the content here; for brevity the notebook file is not embedded fully—copy the patterns from the scripts.)

notebooks/04_Modeling_advanced.ipynb (skeleton)

Includes:

Feature matrix overview (from Role B output path)

Train XGBoost and LightGBM with grid search (randomized) within CV windows

LSTM training cell with reproducible seed

Collect metrics and make a final table

How to run (quick)

Prepare a CSV data/train.csv with columns: ds (datetime), y (target), plus any features. If you have only the raw series, create lag features + calendar features in a preprocessing step (or use Role B outputs).

Edit configs/xgboost.yaml to point at your data and desired hyperparameters.

Train XGBoost:

python src/models/train_xgboost.py --config configs/xgboost.yaml

This will run rolling CV, save CV logs to logs/experiments.csv, final trained model to models/ and metrics summary to results/metrics/baseline_vs_models.csv.

Notes, assumptions and next steps

The scripts assume a tabular feature matrix. If you only have a raw series, create lag features (y_lag_1, y_lag_12), rolling means, time features (month, dayofweek) — you can do this in a preprocessing script or notebook.

SARIMA
